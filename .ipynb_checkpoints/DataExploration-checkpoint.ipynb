{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = pd.read_csv(\"train.csv\" )# reading spam tweets and dropping those with Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          ['goshens', 'crispy', 'salmon', 'sushi', 'url']\n",
      "1        ['new', 'post', 'url', 'near', 'orbit', 'vangu...\n",
      "2        ['merry', 'christmas', 'from', 'léquipe', 'pac...\n",
      "3        ['video', 'everywhere', 'why', 'not', 'email',...\n",
      "4        ['americans#', 'health', 'both', 'improved', '...\n",
      "5        ['thcentury', 'british', 'coaching', 'inn', 'g...\n",
      "6        ['mentionsjen', 'doesnt', 'have', 'iphone', 'h...\n",
      "7        ['motorola', 'buy', 'super', 'bowl', 'spot', '...\n",
      "8        ['spring', 'into', 'event', 'across', 'the', '...\n",
      "9        ['matz', 'barbarez', 'rücktritt', 'ist', 'nur'...\n",
      "10       ['url', 'xbox', 'repair', 'guide', 'top', 'rat...\n",
      "11       ['某サイトの被リンクサービスを見ると、ページランク７の場合で、', '募集が３０ｕｒｌ、１...\n",
      "12       ['photo', 'the', 'famous', 'shirt', 'url', 'sh...\n",
      "13       ['update', 'roadskull', 'reunion', 'recorded',...\n",
      "14       ['the', 'master', 'your', 'mind', 'the', 'secr...\n",
      "15       ['create', 'viral', 'stream', 'free', 'targete...\n",
      "16                                ['調度外に出るとき雨降ってない！ラッキー♪']\n",
      "17       ['unique', 'indoor', 'cat', 'tree', 'url', 'lo...\n",
      "18                                 ['share', 'url', 'url']\n",
      "19       ['alô', 'insones', 'faz', 'dia', 'mais', 'dia'...\n",
      "20       ['free', 'movie', 'download', 'the', 'secret',...\n",
      "21       ['minzhu', 'url', 'zhu', 'min', 'named', 'imf'...\n",
      "22       ['le', 'brown', 'you', 'are', 'the', 'only', '...\n",
      "23       ['have', 'your', 'lunch', 'delivered', 'while'...\n",
      "24       ['mariah', 'carey', 'feiert', 'ihre', 'verlieb...\n",
      "25       ['after', 'year', 'the', 'sideline', 'we’re', ...\n",
      "26       ['twitter', 'tip', 'how', 'use', 'twitter', 'g...\n",
      "27       ['week', 'full', 'pizza', 'pasta', 'and', 'pas...\n",
      "28       ['hollywood', 'dont', 'the', 'whole', 'family'...\n",
      "29       ['jenson', 'button', 'revealed', 'face', 'head...\n",
      "                               ...                        \n",
      "44134    ['getting', 'approved', 'for', 'disability', '...\n",
      "44135    ['want', 'more', 'follower', 'then', 'you', 'n...\n",
      "44136    ['healthcare', 'decision', 'support', 'case', ...\n",
      "44137    ['update', 'the', 'complete', 'guide', 'profit...\n",
      "44138    ['now', 'playing', 'thong', 'song', '#sisqo', ...\n",
      "44139    ['mercado', 'milan', 'ficha', 'joven', 'porter...\n",
      "44140            ['sleep', 'tight', 'mentionantripratiwi']\n",
      "44141    ['elizabeth', 'arden', 'custom', 'color', 'fou...\n",
      "44142    ['just', 'took', 'that', 'what', 'celeb', 'you...\n",
      "44143    ['vorarlberger', 'banken', 'verleihen', 'mehr'...\n",
      "44144    ['live', 'performance', 'url', 'feat', 'hayley...\n",
      "44145    ['kathimerino', 'freerol', 'mono', 'gia', 'ell...\n",
      "44146    ['made', 'fist', 'fight', 'lie', 'win', 'publi...\n",
      "44147    ['cryptor', 'secures', 'individual', 'file', '...\n",
      "44148    ['starcraft', 'demo', 'and', 'free', 'beta', '...\n",
      "44149    ['sean', 'penn', 'senate', 'haiti', 'razor', '...\n",
      "44150                             ['mentionombie_', 'why']\n",
      "44151    ['mentionixiepansy', 'why', 'doe', 'every', 'r...\n",
      "44152    ['new', 'blog', 'post', 'report', 'britney', '...\n",
      "44153    ['make', 'money', 'autopilot', 'see', 'the', '...\n",
      "44154    ['guy', 'let', 'vote', 'for', 'mentioneedictat...\n",
      "44155    ['new', 'blog', 'post', 'best', 'dita', 'von',...\n",
      "44156    ['express', 'our', 'gratitude', 'must', 'never...\n",
      "44157    ['learn', 'the', 'best', 'way', 'make', 'money...\n",
      "44158    ['b#th', 'mentionnetcarblog', 'porsche', 'plug...\n",
      "44159    ['relax', 'get', 'comfortable', 'close', 'your...\n",
      "44160    ['uncontrollable', 'ego', 'may', 'require', 'd...\n",
      "44161    ['wealth', 'the', 'ability', 'fully', 'experie...\n",
      "44162    ['ce', 'hdtvapalooza', 'yellow', 'pixel', 'and...\n",
      "44163    ['mentionkywaitress', 'thanks', 'for', 'the', ...\n",
      "Name: x, Length: 44164, dtype: object\n"
     ]
    }
   ],
   "source": [
    "spam = spam_df['x']\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353468"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now try to look at the amount of URLs and mentions in the Spam Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620227\n"
     ]
    }
   ],
   "source": [
    "urls = 0\n",
    "for tweet in spam:\n",
    "    try:\n",
    "        if re.search(\"http\", tweet) != None:\n",
    "            urls = urls + 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around 68.84423327616946% of spam tweets have URLs in them\n"
     ]
    }
   ],
   "source": [
    "print(\"Around {}% of spam tweets have URLs in them\".format(urls/len(spam)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411401\n",
      "Around 17.48062858725931% of spam tweets have mentions in them\n"
     ]
    }
   ],
   "source": [
    "mentions = 0\n",
    "for tweet in spam:\n",
    "    try:\n",
    "        if re.search(\"@\", tweet) != None:\n",
    "            mentions += 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(mentions)\n",
    "print(\"Around {}% of spam tweets have mentions in them\".format(mentions/len(spam)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Lexical Richness* ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will try to tokenize the tweets and perform basic cleaning on them ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "lm = WordNetLemmatizer()\n",
    "import re\n",
    "\n",
    "# converts symbols from nltk format to wordnet format.\n",
    "# Taken from stackoverflow https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MELBOURNE ENQUIRY: Seeking a variety of acts for our end of year show. Payment is $120 per slot or $200 for 2.... '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',spam[0])\n",
    "# Removed the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',\"www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MELBOURNE ENQUIRY Seeking a variety of acts for our end of year show Payment is  per slot or  for  httpbitlyAhfF'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[\\.\\\\/\\(\\),\\-!@#$%^&*~`:0-9\\?]', '', spam[0])\n",
    "# Removes all the special characters I could think of (including numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list \n",
    "def clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',tweet)\n",
    "    tweet = re.sub(r'[\\.\\\\/\\(\\),\\-!@#$%^&*~`:0-9\\?+=-\\[\\]\\{\\};\\'\\\"<>]', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    tweet = nltk.pos_tag(tweet) # adds the \"part of sentence (noun, verb, adjective, etc )\" after each word\n",
    "    tweet = [lm.lemmatize(x[0], get_wordnet_pos(x[1])) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_vocab = set()\n",
    "for tweet in spam[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    spam_vocab.update(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ttr = []\n",
    "for tweet in spam[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    try:\n",
    "        ttr.append(len(set(tweet))/len(tweet))\n",
    "    except:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8947368421052632,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.9333333333333333,\n",
       " 0.9230769230769231]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954053551603292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try the same on not-spam tweets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = pd.read_csv('spam_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ham['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673166\n"
     ]
    }
   ],
   "source": [
    "urls = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"http\", tweet) != None:\n",
    "            urls = urls + 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around 20.651221235387034% of spam tweets have URLs in them\n"
     ]
    }
   ],
   "source": [
    "print(\"Around {}% of ham tweets have URLs in them\".format(urls/len(tweets)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a big drop compared to almost 70 % in spam tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489738\n",
      "Around 45.70181652187278% of ham tweets have mentions in them\n"
     ]
    }
   ],
   "source": [
    "mentions = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"@\", tweet) != None:\n",
    "            mentions += 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(mentions)\n",
    "print(\"Around {}% of ham tweets have mentions in them\".format(mentions/len(tweets)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that spam tweets actually have less mentions than those not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ttr = []\n",
    "for tweet in tweets[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    try:\n",
    "        ttr.append(len(set(tweet))/len(tweet))\n",
    "    except:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664079833392205"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TTR ratio only seems to be slightly higher in ham than spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725\n"
     ]
    }
   ],
   "source": [
    "ham_vocab = set()\n",
    "for tweet in tweets[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    ham_vocab.update(tweet)\n",
    "    \n",
    "print(len(ham_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vocabulary itself seems to be much richer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham vocab is 49.05962384953982% larger than spam vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"Ham vocab is {}% larger than spam vocab\".format((3725-2499)/2499*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we can try to see the diversity and similarity between the words used in the two vocabs using a pretrained word2vec model ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
