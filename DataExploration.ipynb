{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = pd.read_csv(\"training_tweets.csv\",encoding='latin-1' )\n",
    "tweets=spam_df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          is upset that he can't update his Facebook by ...\n",
      "1          @Kenichan I dived many times for the ball. Man...\n",
      "2            my whole body feels itchy and like its on fire \n",
      "3          @nationwideclass no, it's not behaving at all....\n",
      "4                              @Kwesidei not the whole crew \n",
      "5                                                Need a hug \n",
      "6          @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "7                       @Tatiana_K nope they didn't have it \n",
      "8                                  @twittera que me muera ? \n",
      "9                spring break in plain city... it's snowing \n",
      "10                                I just re-pierced my ears \n",
      "11         @caregiving I couldn't bear to watch it.  And ...\n",
      "12         @octolinz16 It it counts, idk why I did either...\n",
      "13         @smarrison i would've been the first, but i di...\n",
      "14         @iamjazzyfizzle I wish I got to watch it with ...\n",
      "15         Hollis' death scene will hurt me severely to w...\n",
      "16                                      about to file taxes \n",
      "17         @LettyA ahh ive always wanted to see rent  lov...\n",
      "18         @FakerPattyPattz Oh dear. Were you drinking ou...\n",
      "19         @alydesigns i was out most of the day so didn'...\n",
      "20         one of my friend called me, and asked to meet ...\n",
      "21          @angry_barista I baked you a cake but I ated it \n",
      "22                    this week is not going as i had hoped \n",
      "23                                blagh class at 8 tomorrow \n",
      "24            I hate when I have to call and wake people up \n",
      "25         Just going to cry myself to sleep after watchi...\n",
      "26                                    im sad now  Miss.Lilly\n",
      "27         ooooh.... LOL  that leslie.... and ok I won't ...\n",
      "28         Meh... Almost Lover is the exception... this t...\n",
      "29         some1 hacked my account on aim  now i have to ...\n",
      "                                 ...                        \n",
      "1048545    @SteviBianca keep quoting Soulja Boy and i WIL...\n",
      "1048546    zomg, my previous tweet was my 101th!  movie s...\n",
      "1048547                   I saw @paigejavier at Mary Grace! \n",
      "1048548    1 thing I like about the subway: randomly meet...\n",
      "1048549       @GillDeCosemo Thank you for the follow friday \n",
      "1048550                       and this  http://bit.ly/19bJGz\n",
      "1048551                                          @ItsJustDi \n",
      "1048552         Holy Only 12 fucking days till i see callum \n",
      "1048553    @sonnycentral Thank you! I did  Have a great day!\n",
      "1048554    @JonathanRKnight You have the same sense of hu...\n",
      "1048555    I'm eating cheezits...with TWO flavors!  sharp...\n",
      "1048556    TGIF snitches!!....kids acting chill...its pay...\n",
      "1048557    Going to class till noon.....Jammin out to the...\n",
      "1048558    #followfriday because he is as sick in the hea...\n",
      "1048559    watchin Espn's First Take! my favorite mornin ...\n",
      "1048560    is showering.  Then off to Robinson for a litt...\n",
      "1048561    Muhaha! ThankGoodness I missed RMCAAT2009 last...\n",
      "1048562    Good morning people of twitter. TGIFriday! Tha...\n",
      "1048563    Just sitting in the garden leting the sun do i...\n",
      "1048564                        in tampa... going to see him \n",
      "1048565    Looking forward to a mini-break in Isle of Wig...\n",
      "1048566    GRINGO STAR tonight. Southern garage.  http://...\n",
      "1048567    @DavidBass hee hee...I'll take rain over wind ...\n",
      "1048568    today's message in the church service was deli...\n",
      "1048569    Back home, thought I'd done for the week, but ...\n",
      "1048570             My GrandMa is making Dinenr with my Mum \n",
      "1048571    Mid-morning snack time... A bowl of cheese noo...\n",
      "1048572    @ShaDeLa same here  say it like from the Termi...\n",
      "1048573               @DestinyHope92 im great thaanks  wbuu?\n",
      "1048574                 cant wait til her date this weekend \n",
      "Name: tweets, Length: 1048575, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38477\n"
     ]
    }
   ],
   "source": [
    "urls = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"http\", tweet) != None:\n",
    "            urls = urls + 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443711\n"
     ]
    }
   ],
   "source": [
    "mentions = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"@\", tweet) != None:\n",
    "            mentions += 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Lexical Richness* ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will try to tokenize the tweets and perform basic cleaning on them ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "lm = WordNetLemmatizer()\n",
    "import re\n",
    "\n",
    "# converts symbols from nltk format to wordnet format.\n",
    "# Taken from stackoverflow https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list \n",
    "def clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'@\\w+', '@mention', tweet)\n",
    "    tweet = re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '@url',tweet)\n",
    "    tweet = re.sub(r'[\\.\\\\/\\(\\),\\-$%^&*~`:0-9\\+=-\\[\\]\\{\\};\\'\\\"<>]', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    tweet = nltk.pos_tag(tweet) # adds the \"part of sentence (noun, verb, adjective, etc )\" after each word\n",
    "    tweet = [lm.lemmatize(x[0], get_wordnet_pos(x[1])) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list=[]\n",
    "for tweet in tweets[:1000]:        \n",
    "    tweet_list.append(clean(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['be', 'upset', 'that', 'he', 'cant', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'a', 'a', 'result', 'school', 'today', 'also', 'blah!'], ['mention', 'i', 'dive', 'many', 'time', 'for', 'the', 'ball', 'manage', 'to', 'save', 'the', 'rest', 'go', 'out', 'of', 'bound'], ['my', 'whole', 'body', 'feel', 'itchy', 'and', 'like', 'it', 'on', 'fire'], ['mention', 'no', 'it', 'not', 'behave', 'at', 'all', 'im', 'mad', 'why', 'be', 'i', 'here', 'because', 'i', 'cant', 'see', 'you', 'all', 'over', 'there'], ['mention', 'not', 'the', 'whole', 'crew'], ['need', 'a', 'hug'], ['mention', 'hey', 'long', 'time', 'no', 'see!', 'yes', 'rain', 'a', 'bit', 'only', 'a', 'bit', 'lol', 'im', 'fine', 'thanks', 'hows', 'you'], ['mention', 'nope', 'they', 'didnt', 'have', 'it'], ['mention', 'que', 'me', 'muera'], ['spring', 'break', 'in', 'plain', 'city', 'it', 'snowing']]\n"
     ]
    }
   ],
   "source": [
    "print(tweet_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
